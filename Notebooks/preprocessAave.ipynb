{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cca3c5e6-414e-424f-a98c-08d6f438d571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyreadr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a43c632d-8194-4897-a8cb-f7ee52e1fa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/data/IDEA_DeFi_Research/LTM/Data/Lending_Protocols/Aave/V2/Mainnet\"\n",
    "\n",
    "include_user_features = True\n",
    "include_time_features = True\n",
    "include_market_features = True\n",
    "include_exo_features = True\n",
    "\n",
    "feature_extension = \"\"\n",
    "if include_user_features:\n",
    "    feature_extension += \"_user\"\n",
    "if include_market_features:\n",
    "    feature_extension += \"_market\"\n",
    "if include_time_features:\n",
    "    feature_extension += \"_time\"\n",
    "if include_exo_features:\n",
    "    feature_extension += \"_exoLagged\"\n",
    "\n",
    "\n",
    "\n",
    "file_path = f\"{data_path}/transactions_user_market_time_exoLagged.rds\"\n",
    "train_path = f\"{data_path}/transactions{feature_extension}_train.csv\"\n",
    "test_path = f\"{data_path}/transactions{feature_extension}_test.csv\"\n",
    "seq_len=10\n",
    "train_test_thres_year=2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bfbdb5d-8237-477f-8aac-cf4d0530e017",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fullData = pyreadr.read_r(file_path)[None]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfda6448-9176-4290-9997-ebae98ad9e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the unnecessary columns:\n",
    "# First, we can definitely drop any columns for which ALL values are NA:\n",
    "all_na_columns = fullData.columns[fullData.isna().all()].tolist()\n",
    "\n",
    "# We build the list of user-level features here, in case we want to drop them easily for some experiments:\n",
    "user_features = [col for col in fullData.columns if col.startswith('user') and col != \"user\"]\n",
    "\n",
    "# We build the list of time features in case we want to drop them easily for some experiments:\n",
    "time_features = [\"timeOfDay\", \"dayOfWeek\", \"dayOfMonth\", \n",
    "    \"dayOfYear\", \"quarter\", \"dayOfQuarter\",\n",
    "    \"sinTimeOfDay\", \"cosTimeOfDay\", \"sinDayOfWeek\",\n",
    "    \"cosDayOfWeek\", \"sinDayOfMonth\", \"cosDayOfMonth\",\n",
    "    \"sinDayOfQuarter\", \"cosDayOfQuarter\", \"sinDayOfYear\",\n",
    "    \"cosDayOfYear\", \"sinQuarter\", \"cosQuarter\", \"isWeekend\"]\n",
    "\n",
    "# We are going to drop the market features for now, because they are similar to exogenous\n",
    "# features and we will likely handle them differently later on.\n",
    "market_features = [col for col in fullData.columns if col.startswith('market')]\n",
    "\n",
    "# We are going to drop the exogenous features for now and handle them separately once we have \n",
    "# the rest of the model working.\n",
    "exo_features = [col for col in fullData.columns if col.startswith('exo')]\n",
    "\n",
    "# There might be additional, hand-selected columns we want to drop just because they are not useful or because\n",
    "# they are going to be re-created in a more general way:\n",
    "other_columns_to_drop = ['logAmountUSD', 'logAmount', 'logAmountETH']\n",
    "\n",
    "# Concatenate these lists and then drop the columns:\n",
    "columns_to_drop = all_na_columns + other_columns_to_drop\n",
    "if not include_user_features:\n",
    "    columns_to_drop += user_features\n",
    "if not include_market_features:\n",
    "    columns_to_drop += market_features\n",
    "if not include_time_features:\n",
    "    columns_to_drop += time_features\n",
    "if not include_exo_features:\n",
    "    columns_to_drop += exo_features\n",
    "\n",
    "data = fullData.drop(columns=columns_to_drop, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66951840-1bad-4234-9dc6-9118dcd5be7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['timeFeature'] = ((data['timestamp'] - data['timestamp'].min())//60).astype(int)\n",
    "data['Year'] = pd.to_datetime(data['timestamp'], unit='s').dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28dadd2d-86d3-4349-b815-d14a7abd5782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make sure certain columns are cast properly to categorical columns so they \n",
    "# are handled appropriately when building the vocabulary down the line.\n",
    "categorical_columns = ['fromState', 'toState', 'borrowRateMode', 'reserve', 'type', \n",
    "                       'collateralReserve', 'borrowRateModeTo', 'borrowRateModeFrom',\n",
    "                       'coinType', \n",
    "                       \"dayOfWeek\", \"dayOfMonth\", \"dayOfYear\", \"quarter\", \"dayOfQuarter\", 'isWeekend', 'Year',\n",
    "                       'userReserveMode', 'userCoinTypeMode', 'userIsNew']\n",
    "\n",
    "for col in categorical_columns:\n",
    "    data[col] = data[col].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c2c5003-d536-4520-acc4-f6b2ee0f5ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_columns =  ['user', 'timeFeature']\n",
    "new_data = data.sort_values(by=sort_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f5fb5e-24e7-4f5b-8031-986a6da136c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns we specifically don't want to log-transform:\n",
    "columns_not_to_log = ['timeFeature']\n",
    "\"\"\"\n",
    "# List of keywords indicating a column should be log-transformed\n",
    "keywords_to_log = ['amount', 'sum', 'rate', 'count', 'activedays', 'sin', 'cos']\n",
    "\n",
    "# Identify columns to log-transform\n",
    "columns_to_log = [col for col in new_data.columns if any(keyword in col.lower() for keyword in keywords_to_log)]\n",
    "columns_to_log = list(set(columns_to_log))  # Remove duplicates\n",
    "columns_to_log = [x for x in columns_to_log if isinstance(x, (int, float))]\n",
    "\"\"\"\n",
    "\n",
    "# We want to log-transform all numeric columns:\n",
    "columns_to_log = new_data.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# Apply log transformation to identified columns\n",
    "for col in columns_to_log:\n",
    "    if col in columns_not_to_log:\n",
    "        continue\n",
    "    new_data[col] = new_data[col].apply(lambda x: np.log(x) if x > 0 else np.nan)  # Use 1 for non-positive values\n",
    "    \n",
    "new_data['rowNumber'] = np.arange(len(new_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aed41b9-822d-431d-b7eb-d5956aa6c485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to move to the front\n",
    "first_cols = ['rowNumber', 'user', 'timestamp', 'id']\n",
    "# Rearrange the columns\n",
    "rearranged_columns = first_cols + [col for col in new_data.columns if col not in first_cols]\n",
    "new_data = new_data[rearranged_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9c5721-2393-45b1-a4df-e5c4a9ca3066",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = new_data.loc[new_data['Year'].astype(int) < train_test_thres_year]\n",
    "basic_test_data = new_data.loc[new_data['Year'].astype(int) >= train_test_thres_year]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5c7fcc-1e8b-4376-9b42-3913435354c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user = set(train_data['user'].unique())\n",
    "test_user = set(basic_test_data['user'].unique())\n",
    "train_test_user = train_user.intersection(test_user)\n",
    "test_only_user = test_user.difference(train_user)\n",
    "groupby_columns = ['user']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23dd705-dbd6-4aa7-ad2c-b877f793f747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(x, seq_len):\n",
    "    return x.index[-(seq_len-1):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223cc01e-71cf-4984-b1d1-07ae47d7ecf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_extra_index = train_data.loc[train_data['user'].isin(train_test_user)].groupby(groupby_columns).apply(get_index, seq_len)\n",
    "test_extra_index = test_extra_index.explode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7131ca2b-7d10-428f-8157-55a82d4cd41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.concat([new_data.loc[test_extra_index], basic_test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa736a4c-a29f-4132-a419-dcb9d38cc167",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.sort_values(by=sort_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f165a66c-fc7b-4363-962d-d61809ef6be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv(train_path, index=False)\n",
    "test_data.to_csv(test_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (greena12)",
   "language": "python",
   "name": "greena12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
