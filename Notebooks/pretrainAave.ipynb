{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f04e9366-1885-4d8d-b0d0-ede0fd865ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/greena12/.conda/envs/greena12/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "from os import makedirs\n",
    "from os.path import join, basename\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from arguments import define_new_main_parser\n",
    "import json\n",
    "\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "\n",
    "from dataset.aave import AaveDataset\n",
    "from models.modules import TabFormerBertLM, TabFormerBertForClassification, TabFormerBertModel, TabStaticFormerBert, \\\n",
    "    TabStaticFormerBertLM, TabStaticFormerBertClassification\n",
    "from misc.utils import ordered_split_dataset, compute_cls_metrics\n",
    "from dataset.datacollator import *\n",
    "from main import main\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c71d25b3-2692-45ce-a47e-40eb56554314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-22 13:07:29,791 - root - INFO - Logging setup completed.\n"
     ]
    }
   ],
   "source": [
    "def setup_logging(output_dir=\"logs\", log_file_name='output.log'):\n",
    "    log_dir = join(output_dir, \"logs\")\n",
    "    makedirs(output_dir, exist_ok=True)\n",
    "    makedirs(log_dir, exist_ok=True)\n",
    "    log_file = join(log_dir, log_file_name)\n",
    "\n",
    "    logger = logging.getLogger()\n",
    "\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "\n",
    "    fhandler = logging.FileHandler(log_file)\n",
    "    fhandler.setLevel(logging.DEBUG)\n",
    "\n",
    "    chandler = logging.StreamHandler()\n",
    "    chandler.setLevel(logging.DEBUG)\n",
    "\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    fhandler.setFormatter(formatter)\n",
    "    chandler.setFormatter(formatter)\n",
    "\n",
    "    logger.addHandler(fhandler)\n",
    "    logger.addHandler(chandler)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    return logger\n",
    "\n",
    "logger = setup_logging(output_dir=\"logs\")\n",
    "\n",
    "\n",
    "logger.info(\"Logging setup completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36069bb4-c991-412d-adb9-12c6470427f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "include_user_features = True\n",
    "include_time_features = True\n",
    "include_market_features = True\n",
    "include_exo_features = True\n",
    "\n",
    "feature_extension = \"\"\n",
    "if include_user_features:\n",
    "    feature_extension += \"_user\"\n",
    "if include_market_features:\n",
    "    feature_extension += \"_market\"\n",
    "if include_time_features:\n",
    "    feature_extension += \"_time\"\n",
    "if include_exo_features:\n",
    "    feature_extension += \"_exoLagged\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f22c9d91-648d-4bc4-8f95-2d9acddae6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=\"/data/IDEA_DeFi_Research/LTM/Data/Lending_Protocols/Aave/V2/Mainnet\" \n",
    "dt=\"Aave\"\n",
    "exp_name=\"debug\"\n",
    "time_pos_type=\"regular_position\"\n",
    "fname = f\"transactions{feature_extension}_train\"  \n",
    "val_fname = f\"transactions{feature_extension}_val\" \n",
    "test_fname = f\"transactions{feature_extension}_test\"  \n",
    "fextension = False\n",
    "bs=32\n",
    "field_hs = 64 # hidden state dimension of the transformer (default: 768)\n",
    "seq_len = 20 # length for transaction sliding window\n",
    "stride = 5 # stride for transaction sliding window\n",
    "num_train_epochs=10\n",
    "save_steps=100\n",
    "eval_steps=100\n",
    "external_val=False\n",
    "output_dir=f\"{data}/output/{exp_name}\"\n",
    "checkpoint=None\n",
    "nrows=10000\n",
    "vocab_dir=f\"{data}/vocab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0d9a57b-0dd5-4b64-ac65-5e189091bdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_str = f\"--do_train \\\n",
    "    --mlm \\\n",
    "    --pad_seq_first \\\n",
    "    --get_rids \\\n",
    "    --field_ce \\\n",
    "    --lm_type bert \\\n",
    "    --field_hs {field_hs} \\\n",
    "    --data_type {dt} \\\n",
    "    --seq_len {seq_len} \\\n",
    "    --stride {stride} \\\n",
    "    --num_train_epochs {num_train_epochs} \\\n",
    "    --data_root {data}/ \\\n",
    "    --train_batch_size {bs} \\\n",
    "    --eval_batch_size {bs} \\\n",
    "    --save_steps {save_steps} \\\n",
    "    --eval_steps {eval_steps} \\\n",
    "    --data_fname {fname} \\\n",
    "    --data_val_fname {val_fname} \\\n",
    "    --data_test_fname {test_fname} \\\n",
    "    --output_dir {output_dir} \\\n",
    "    --time_pos_type {time_pos_type} \\\n",
    "    --vocab_dir {vocab_dir} \\\n",
    "    --nrows {nrows} \\\n",
    "    --vocab_cached \\\n",
    "    --encoder_cached \\\n",
    "    --cached \\\n",
    "    \"\n",
    "   # \n",
    "if fextension:\n",
    "    arg_str += f\"--fextension {fextension} \\\n",
    "    --external_vocab_path {data}/vocab_ob_{fextension}\"\n",
    "else:\n",
    "    arg_str += f\"--external_vocab_path {data}/vocab/vocab_ob\"\n",
    "if external_val:\n",
    "    arg_str += f\"\\\n",
    "    --external_val\"\n",
    "if checkpoint is not None:\n",
    "    arg_str += f\"\\\n",
    "    --checkpoint {checkpoint}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06320fe2-d4bf-41ff-ba54-1223c6f77d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--do_train     --mlm     --pad_seq_first     --get_rids     --field_ce     --lm_type bert     --field_hs 64     --data_type Aave     --seq_len 20     --stride 5     --num_train_epochs 10     --data_root /data/IDEA_DeFi_Research/LTM/Data/Lending_Protocols/Aave/V2/Mainnet/     --train_batch_size 32     --eval_batch_size 32     --save_steps 100     --eval_steps 100     --data_fname transactions_user_market_time_train     --data_val_fname transactions_user_market_time_val     --data_test_fname transactions_user_market_time_test     --output_dir /data/IDEA_DeFi_Research/LTM/Data/Lending_Protocols/Aave/V2/Mainnet/output/debug     --time_pos_type regular_position     --vocab_dir /data/IDEA_DeFi_Research/LTM/Data/Lending_Protocols/Aave/V2/Mainnet/vocab     --nrows 10000     --vocab_cached     --encoder_cached     --cached     --external_vocab_path /data/IDEA_DeFi_Research/LTM/Data/Lending_Protocols/Aave/V2/Mainnet/vocab/vocab_ob\n"
     ]
    }
   ],
   "source": [
    "parser = define_new_main_parser(data_type_choices=[\"Aave\"])\n",
    "print(arg_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9615562a-98c2-4f55-b1ae-38490a5a0e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(jid=1, seed=42, output_dir='/data/IDEA_DeFi_Research/LTM/Data/Lending_Protocols/Aave/V2/Mainnet/output/debug', lm_type='bert', flatten=False, field_ce=True, mlm=True, cls_task=False, export_task=False, export_last_only=False, mlm_prob=0.15, freeze=False, data_type='Aave', time_pos_type='regular_position', data_root='/data/IDEA_DeFi_Research/LTM/Data/Lending_Protocols/Aave/V2/Mainnet/', data_fname='transactions_user_market_time_train', vocab_file='vocab.nb', user_ids=None, vocab_cached=True, external_encoder_fname='./data/preprocessed/transactionsAave_train.encoder_fit.pkl', external_vocab_fname='./data/vocab_ob', nrows=10000, label_category='last_label', nbatches=None, record_file='experiments', pretrained_dir=None, vocab_dir='/data/IDEA_DeFi_Research/LTM/Data/Lending_Protocols/Aave/V2/Mainnet/vocab', checkpoint=0, do_train=True, do_eval=False, do_prediction=False, save_steps=100, eval_steps=100, num_train_epochs=10, train_batch_size=32, eval_batch_size=32, stride=5, seq_len=20, field_hs=64, skip_user=False, pad_seq_first=True, get_rids=True, long_and_sort=False, user_level_cached=False, external_vocab_path='/data/IDEA_DeFi_Research/LTM/Data/Lending_Protocols/Aave/V2/Mainnet/vocab/vocab_ob', preload_fextension=None, fextension='', data_val_fname='transactions_user_market_time_val', data_test_fname='transactions_user_market_time_test', resample_method=None, resample_ratio=10, resample_seed=100, external_val=False, encoder_cached=True, cached=True)\n"
     ]
    }
   ],
   "source": [
    "opts = parser.parse_args(arg_str.split())\n",
    "print(opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b4a79b5-cb02-4b9e-9636-2388d9ede926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-22 13:07:29,874 - dataset.aave_basic - INFO - cached encoded data is read from transactions_user_market_time_train.encoded.csv\n",
      "2025-01-22 13:07:30,054 - dataset.aave_basic - INFO - read data : (10000, 128)\n",
      "2025-01-22 13:07:30,057 - dataset.aave_basic - INFO - using cached vocab from /data/IDEA_DeFi_Research/LTM/Data/Lending_Protocols/Aave/V2/Mainnet/vocab/vocab_ob\n",
      "2025-01-22 13:07:30,070 - dataset.aave - INFO - preparing user level data...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:00<00:00, 40.83it/s]\n",
      "2025-01-22 13:07:30,553 - dataset.aave - INFO - creating transaction samples with vocab\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:00<00:00, 32.58it/s]\n",
      "2025-01-22 13:07:31,109 - dataset.aave - INFO - ncols: 125\n",
      "2025-01-22 13:07:31,110 - dataset.aave - INFO - no of samples 2008\n",
      "2025-01-22 13:07:31,120 - main - INFO - vocab size: 1732\n",
      "2025-01-22 13:07:31,121 - main - INFO - dataset size: 2008\n",
      "2025-01-22 13:07:31,147 - dataset.aave_basic - INFO - cached encoded data is read from transactions_user_market_time_train.encoded.csv\n",
      "2025-01-22 13:07:31,195 - dataset.aave_basic - INFO - read data : (10000, 128)\n",
      "2025-01-22 13:07:31,196 - dataset.aave_basic - INFO - using cached vocab from /data/IDEA_DeFi_Research/LTM/Data/Lending_Protocols/Aave/V2/Mainnet/vocab/vocab_ob\n",
      "2025-01-22 13:07:31,201 - dataset.aave - INFO - preparing user level data...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:00<00:00, 44.23it/s]\n",
      "2025-01-22 13:07:31,644 - dataset.aave - INFO - creating transaction samples with vocab\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:00<00:00, 36.42it/s]\n",
      "2025-01-22 13:07:32,141 - dataset.aave - INFO - ncols: 125\n",
      "2025-01-22 13:07:32,141 - dataset.aave - INFO - no of samples 2008\n",
      "2025-01-22 13:07:32,146 - main - INFO - test dataset size: 2008\n",
      "2025-01-22 13:07:32,147 - main - INFO - # Using external test dataset, lengths: train [1686]  valid [322]  test [2008]\n",
      "2025-01-22 13:07:32,147 - main - INFO - # lengths: train [0.42]  valid [0.08]  test [0.50]\n",
      "/home/greena12/.conda/envs/greena12/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "2025-01-22 13:08:05,302 - main - INFO - model initiated: <class 'models.modules.TabFormerHierarchicalLM'>\n",
      "2025-01-22 13:08:05,305 - main - INFO - Total parameters: 3745390660\n",
      "2025-01-22 13:08:05,305 - main - INFO - Trainable parameters: 3745390660\n",
      "2025-01-22 13:08:05,306 - main - INFO - collator class: TransDataCollatorForLanguageModeling\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m opts\u001b[38;5;241m.\u001b[39mmlm) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m opts\u001b[38;5;241m.\u001b[39mcls_exp_task) \u001b[38;5;129;01mand\u001b[39;00m opts\u001b[38;5;241m.\u001b[39mlm_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: Bert needs either \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--mlm\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--cls_task\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--export_task\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m option. Please re-run with this flag \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincluded.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LTM-Encoder-Models/main.py:349\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    347\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mtrain(model_path)\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 349\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# test_preds, test_labels, test_metrics = trainer.predict(test_dataset=test_dataset)\u001b[39;00m\n\u001b[1;32m    352\u001b[0m test_results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mpredict(test_dataset\u001b[38;5;241m=\u001b[39mtest_dataset)\n",
      "File \u001b[0;32m~/.conda/envs/greena12/lib/python3.10/site-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/greena12/lib/python3.10/site-packages/transformers/trainer.py:2388\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2388\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2391\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2392\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2393\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2394\u001b[0m ):\n\u001b[1;32m   2395\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2396\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.conda/envs/greena12/lib/python3.10/site-packages/transformers/trainer.py:3518\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3516\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3518\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/.conda/envs/greena12/lib/python3.10/site-packages/accelerate/accelerator.py:2241\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/greena12/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/greena12/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/greena12/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "opts.log_dir = join(opts.output_dir, \"logs\")\n",
    "makedirs(opts.output_dir, exist_ok=True)\n",
    "makedirs(opts.log_dir, exist_ok=True)\n",
    "\n",
    "opts.cls_exp_task = opts.cls_task or opts.export_task\n",
    "\n",
    "if opts.data_type in [\"Aave\"]:\n",
    "    assert opts.time_pos_type in ['sin_cos_position', 'regular_position']\n",
    "\n",
    "if (not opts.mlm) and (not opts.cls_exp_task) and opts.lm_type == \"bert\":\n",
    "    raise Exception(\n",
    "        \"Error: Bert needs either '--mlm', '--cls_task' or '--export_task' option. Please re-run with this flag \"\n",
    "        \"included.\")\n",
    "\n",
    "main(opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a833476-ea5c-408d-84f3-711296f21715",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (greena12)",
   "language": "python",
   "name": "greena12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
