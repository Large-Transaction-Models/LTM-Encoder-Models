{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b876f969-9b63-48ee-a624-edfb0647349e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "from models.modules import TabFormerBertLM, TabFormerBertModel, TabFormerHierarchicalLM\n",
    "from dataset.vocab import Vocabulary\n",
    "import pickle\n",
    "import torch\n",
    "from transformers import BertConfig\n",
    "\n",
    "def load_pretrained_model(model_dir, vocab_path):\n",
    "    vocab = pickle.load(open(vocab_path, \"rb\"))\n",
    "    # Step 1: Load the config\n",
    "    config_path = f\"{model_dir}/config.json\"\n",
    "    config = BertConfig.from_json_file(config_path)\n",
    "\n",
    "    # Step 2: Initialize the model\n",
    "    model = TabFormerHierarchicalLM(config, vocab)\n",
    "\n",
    "    # Step 3: Load the model weights\n",
    "    model_weights_path = f\"{model_dir}/pytorch_model.bin\"\n",
    "    state_dict = torch.load(model_weights_path, map_location=torch.device(\"cpu\"))\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f5e19adf-d797-4609-b887-0ac38c012688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cls_embedding(model, input_tokens, vocab):\n",
    "    # Convert input_tokens (list of token IDs) to a PyTorch tensor\n",
    "    input_tensor = torch.tensor([input_tokens], dtype=torch.long)  # Add batch dimension\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)  # Pass tensor to model\n",
    "\n",
    "    cls_embedding = outputs[0][:, 0, :]  # Extract CLS token embedding\n",
    "    return cls_embedding.squeeze(0).numpy()  # Convert to NumPy array for easy use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "74832863-d273-4627-8815-626a92d67fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary data_path\n",
    "data_path = \"/data/IDEA_DeFi_Research/Data/eCommerce/Cosmetics/\"\n",
    "\n",
    "exp_name = \"debug\"\n",
    "model_path = data_path + f\"preprocessed/output/{exp_name}/final-model\"\n",
    "vocab_path = data_path + \"preprocessed/vocab/vocab_ob\"\n",
    "\n",
    "model = load_pretrained_model(model_path, vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dd405bbe-f736-4e46-9cec-097bee96526e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pickle file containing transactions, IDs, and columns\n",
    "def load_trans_rids_columns_from_pkl(pkl_path):\n",
    "    with open(pkl_path, \"rb\") as f:\n",
    "        data = pickle.load(f)  # Expecting a dictionary with \"trans\", \"RIDs\", and \"columns\" keys\n",
    "    \n",
    "    transactions = data[\"trans\"]  # List of tokenized sequences\n",
    "    transaction_ids = data[\"RIDs\"]  # Corresponding transaction ID lists\n",
    "    columns = data[\"columns\"]  # Column names (used for vocab field names)\n",
    "\n",
    "    assert len(transactions) == len(transaction_ids), \"Mismatch between transactions and RIDs\"\n",
    "    \n",
    "    return transactions, transaction_ids, columns\n",
    "\n",
    "# Convert token sequences to embeddings while associating with the correct field\n",
    "def process_and_embed(pkl_path, model, vocab):\n",
    "    transactions, transaction_ids, columns = load_trans_rids_columns_from_pkl(pkl_path)\n",
    "\n",
    "    embeddings_dict = {}  # Store {last_transaction_id: CLS_embedding}\n",
    "\n",
    "    for i in range(len(transactions)):\n",
    "        tokens = transactions[i]  # Tokenized sequence of transactions\n",
    "        last_transaction_id = transaction_ids[i][-1]  # Get the last transaction ID\n",
    "\n",
    "        # Ensure tokens and columns match in length\n",
    "        if len(tokens) != len(columns):\n",
    "            print(f\"Skipping sequence {i}: Token and column count mismatch.\")\n",
    "            continue\n",
    "\n",
    "        # Convert tokens to numerical IDs using the corresponding field names\n",
    "        input_ids = []\n",
    "        for token, field_name in zip(tokens, columns):  # Map each token to its field\n",
    "            try:\n",
    "                token_id = vocab.get_id(token, field_name)  # Use the correct field\n",
    "                input_ids.append(token_id)\n",
    "            except KeyError:\n",
    "                print(f\"Token '{token}' not found in vocab under field '{field_name}'\")\n",
    "\n",
    "        # Compute CLS embedding if valid input exists\n",
    "        if input_ids:\n",
    "            cls_embedding = get_cls_embedding(model, input_ids, vocab)\n",
    "            embeddings_dict[last_transaction_id] = cls_embedding\n",
    "\n",
    "    return embeddings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "865273dd-21f3-4019-9499-8bbb31f908e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (44x64 and 2880x2880)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m data_to_encode_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mpreprocessed/preprocessed/transactions_user_time_test.user.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m cls_embeddings_with_last_id \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_and_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_to_encode_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Print example output\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m trans_id, embedding \u001b[38;5;129;01min\u001b[39;00m cls_embeddings_with_last_id\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[0;32mIn[86], line 40\u001b[0m, in \u001b[0;36mprocess_and_embed\u001b[0;34m(pkl_path, model, vocab)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# Compute CLS embedding if valid input exists\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m input_ids:\n\u001b[0;32m---> 40\u001b[0m         cls_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mget_cls_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m         embeddings_dict[last_transaction_id] \u001b[38;5;241m=\u001b[39m cls_embedding\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings_dict\n",
      "Cell \u001b[0;32mIn[90], line 6\u001b[0m, in \u001b[0;36mget_cls_embedding\u001b[0;34m(model, input_tokens, vocab)\u001b[0m\n\u001b[1;32m      3\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([input_tokens], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)  \u001b[38;5;66;03m# Add batch dimension\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 6\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Pass tensor to model\u001b[39;00m\n\u001b[1;32m      8\u001b[0m cls_embedding \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m][:, \u001b[38;5;241m0\u001b[39m, :]  \u001b[38;5;66;03m# Extract CLS token embedding\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cls_embedding\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/.conda/envs/greena12/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/greena12/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/LTM-Encoder-Models/models/modules.py:72\u001b[0m, in \u001b[0;36mTabFormerHierarchicalLM.forward\u001b[0;34m(self, input_ids, **input_args)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minput_args):\n\u001b[0;32m---> 72\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtab_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtb_model(inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minput_args)\n",
      "File \u001b[0;32m~/.conda/envs/greena12/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/greena12/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/LTM-Encoder-Models/models/hierarchical.py:129\u001b[0m, in \u001b[0;36mTabFormerEmbeddings.forward\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m    126\u001b[0m inputs_embeds \u001b[38;5;241m=\u001b[39m inputs_embeds\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    127\u001b[0m inputs_embeds \u001b[38;5;241m=\u001b[39m inputs_embeds\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(embeds_shape[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m+\u001b[39m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m--> 129\u001b[0m inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlin_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inputs_embeds\n",
      "File \u001b[0;32m~/.conda/envs/greena12/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/greena12/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/greena12/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (44x64 and 2880x2880)"
     ]
    }
   ],
   "source": [
    "data_to_encode_path = f\"{data_path}preprocessed/preprocessed/transactions_user_time_test.user.pkl\"\n",
    "cls_embeddings_with_last_id = process_and_embed(data_to_encode_path, model, vocab)\n",
    "\n",
    "# Print example output\n",
    "for trans_id, embedding in cls_embeddings_with_last_id.items():\n",
    "    print(f\"Last Transaction ID: {trans_id}, CLS Embedding Shape: {embedding.shape}\")\n",
    "    break  # Show one example\n",
    "\n",
    "# Convert to DataFrame for saving\n",
    "df = pd.DataFrame.from_dict(cls_embeddings_with_last_id, orient=\"index\")\n",
    "df.reset_index(inplace=True)\n",
    "df.rename(columns={\"index\": \"transaction_id\"}, inplace=True)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"cls_embeddings_with_ids.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "036c25e7-8f41-4dad-92ca-62103849e7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_to_encode_path, \"rb\") as f:\n",
    "    data = pickle.load(f)  # Expecting a list of dicts or tuples (id, tokens)\n",
    "\n",
    "transaction_ids = []\n",
    "token_sequences = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e2731a9a-e1db-41db-b09f-0c0b01eaf3a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['trans', 'labels', 'RIDs', 'columns'])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4153fed9-e453-4307-8a83-1f03d1364034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13.98140797394406], [14.901737125956725, 14.901737125956725, 14.901737125956725, 14.901737125956725, 14.901737125956725, 14.901737125956725, 14.901737125956725, 14.901737125956725, 14.901737125956725, 14.901737125956725, 14.901737125956725, 14.901737125956725, 14.901737125956725, 14.901737125956725, 14.901737125956725, 14.901737125956725, 14.901737125956725, 14.901737125956725, 14.901737125956725, 14.901737125956725, 14.901737125956725, 14.901737125956725, 14.901737125956725, 14.901737125956725], [15.354779622007266, 15.354779622007266, 15.354779622007266, 15.354779622007266, 15.354779622007266, 15.354779622007266, 15.354779622007266], [15.40303266391096], [16.13551475364073]]\n"
     ]
    }
   ],
   "source": [
    "print(data['labels'][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "54217363-64d9-4bc6-8a44-e8f1bf18275f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15.016693875286428], [14.81294763443448, 14.81295648615924, 14.812960543173569, 14.813022871597544, 14.813067494887552, 14.813082983497893, 14.81308999017138, 14.813143460536365, 14.813197296773918, 14.813226794853042, 14.813255554642437, 14.81329353104582, 14.813383120147126, 14.81348302280663, 14.813490395301178, 14.813526151128691, 14.81352762556836, 14.81353978961266, 14.813558219700884, 14.813560799886131, 14.81356522304535, 14.813566328832096, 14.81368722080863, 14.814296986473032], [13.46599847083523, 14.828855765655373, 14.82887028565504, 14.828872826633305, 14.828879723541766, 14.828881538509773, 14.828885168435898], [14.949156279710778], [14.945504581790887]]\n"
     ]
    }
   ],
   "source": [
    "print(data['RIDs'][0:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (greena12)",
   "language": "python",
   "name": "greena12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
